{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Identification of Vertebral Fractures Using Artificial Intelligence Convolutional Neural Networks Predicts Incident Non-vertebral and Hip Fractures: A Registry-Based Cohort Study\n",
    "\n",
    "Source code for the paper \"Automated Identification of Vertebral Fractures Using Artificial Intelligence Convolutional Neural Networks Predicts Incident Non-vertebral and Hip Fractures: A Registry-Based Cohort Study\".\n",
    "\n",
    "To run the code in your own environment you will require a python 3.6 environment with the following packages installed (more recent versions may work but you'll be on your own):\n",
    "\n",
    "  * h5py 2.7.1\n",
    "  * numpy 1.15.4\n",
    "  * matplotlib 3.0.2\n",
    "  * opencv-python 4.1.0.25\n",
    "  * pandas 0.23.4\n",
    "  * Keras 2.1.5\n",
    "  * scikit-learn 0.19.1\n",
    "  * tensorflow-gpu 1.6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package imports and notebook-wide constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import callbacks, optimizers\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, InputLayer, Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import auc, confusion_matrix, roc_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "height = 600\n",
    "width = int(height * 0.6)\n",
    "\n",
    "base = '/data/VFA'\n",
    "train_dir = os.path.join(base, 'phase_1_all_balanced_up')\n",
    "valid_dir = os.path.join(base, 'phase_2_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load previous model\n",
    "\n",
    "Use these cells to load and verify a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('incresnetv2_600_sgdr_1_10_epochs_all_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new model\n",
    "\n",
    "Create new network model using a base model of Inception-ResNet-v2 with a 'top' of global average pooling and two dense layers for classification. Random initialization is used throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "base_model = InceptionResNetV2(weights=None, include_top=False)\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=1e-4), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation and data generators\n",
    "\n",
    "Keras includes a nice set of functionality for sampling and augmenting model training data. In this instance we'll use Keras' ImageDataGenerator to randomly apply selected transformations to the base data when training. The transformations used in model training are:\n",
    "\n",
    "  * Rotation by up to 30 degrees\n",
    "  * Shifting the image horizontally by up to 20% of the image width\n",
    "  * Shifting the image vertically by up to 20% of the image height\n",
    "  * Shearing the image by up to 0.2 radians\n",
    "  * Increasing/reducing color channel intensity by up to 10%\n",
    "\n",
    "Points outside the original image boundaries (e.g. from shearing) are filled with a constant value of 0, i.e. black. \n",
    "Images are multipled (rescaled in Keras parlance) by 1 / 255 to bring pixel values into the range \\[0, 1\\] before any other transformations are applied. \n",
    "\n",
    "Test and validation data will not be transformed beyond rescaling of pixel values into the range \\[0, 1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30, \n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    channel_shift_range=0.1,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(height, width), \n",
    "    shuffle=True, \n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['norm', 'frac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(height, width),\n",
    "    shuffle=False, \n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes=['norm', 'frac'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visually check image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [16, 16]\n",
    "\n",
    "train_frac = os.path.join(train_dir, 'frac')\n",
    "fnames = [os.path.join(train_frac, fname) for fname in os.listdir(train_frac)]\n",
    "img_path = fnames[4]\n",
    "img = image.load_img(img_path, target_size=(height, width))\n",
    "x = image.img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "i = 1\n",
    "\n",
    "for batch in train_datagen.flow(x, batch_size=1):\n",
    "    plt.subplot(1, 4, i)\n",
    "    plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weights\n",
    "\n",
    "Here we set training weights for the classes to account for the imbalance in the number of examples of each class (no fracture, fracture) in the training data. In our case, the number of examples of the negative class (0, normal) is approximately 4x that of the positive class (1, fracture). Weighting the positive cases by their frequency of occurence relative to the negative case guides the training algorithm to converge to a point that minimizes the network loss (error) for postive cases at potentially the expense of accuracy for negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_counts = np.unique(train_generator.classes, return_counts=True)\n",
    "class_weight = {0: 1., 1: train_counts[0] / train_counts[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medical metrics callback\n",
    "\n",
    "\n",
    "A Keras callback to calculate and report medically relevant model metrics at the completion of each training epoch (complete run through the training data set). Metrics include:\n",
    "\n",
    "  * the false postive rate, true positive rate, and thresholds for the receiver operating characteristic;\n",
    "  * the area under the curve;\n",
    "  * the confusion matrix (true/false positives, true/false negatives);\n",
    "  * the sensitive and specificity of the model;\n",
    "  * the positive and negative predictive values;\n",
    "  * postive and negative likelihood ratios; and\n",
    "  * overall accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med_metrics(labels, predictions, thresh = 0.5):\n",
    "  \n",
    "    md = {}\n",
    "\n",
    "    md['fpr'], md['tpr'], md['th'] = roc_curve(labels, predictions)\n",
    "    md['auc'] = auc(md['fpr'], md['tpr'])\n",
    "\n",
    "    thresh_pred = np.copy(predictions)\n",
    "    thresh_pred[predictions >= thresh] = 1\n",
    "    thresh_pred[predictions < thresh] = 0\n",
    "\n",
    "    md['cm'] = confusion_matrix(labels, thresh_pred)\n",
    "    md['tn'], md['fp'], md['fn'], md['tp'] = md['cm'].ravel()\n",
    "    md['sn'] = md['tp'] / (md['tp'] + md['fn'])\n",
    "    md['sp'] = md['tn'] / (md['tn'] + md['fp'])\n",
    "    md['snspavg'] = (md['sn'] + md['sp']) / 2\n",
    "    md['ppv'] = md['tp'] / (md['tp'] + md['fp'])\n",
    "    md['npv'] = md['tn'] / (md['tn'] + md['fn'])\n",
    "    md['plr'] = md['sn'] / (1 - md['sp'])\n",
    "    md['nlr'] = md['sp'] / (1 - md['sn'])\n",
    "    md['acc'] = (md['tp'] + md['tn']) / (md['tp'] + md['tn'] + md['fp'] + md['fn'])\n",
    "\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedMetricsCallback(callbacks.Callback):\n",
    "    def on_train_begin(self, logs = {}):\n",
    "        self.hist = {'fpr': [],\n",
    "                     'tpr': [],\n",
    "                     'th': [],\n",
    "                     'auc': [],\n",
    "                     'cm': [],\n",
    "                     'tn': [],\n",
    "                     'fp': [],\n",
    "                     'fn': [],\n",
    "                     'tp': [],\n",
    "                     'sn': [],\n",
    "                     'sp': [],\n",
    "                     'snspavg': [],\n",
    "                     'ppv': [],\n",
    "                     'npv': [],\n",
    "                     'plr': [],\n",
    "                     'nlr': [],\n",
    "                     'acc': []\n",
    "                    }\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs = {}):\n",
    "        self.val_pred = model.predict_generator(validation_generator)\n",
    "        self.md = med_metrics(validation_generator.classes, self.val_pred, thresh=0.5)\n",
    "        \n",
    "        for key, value in self.md.items():\n",
    "            self.hist[key].append(value)\n",
    "        \n",
    "        print('\\nVal Sn:', round(self.md['sn'], 3),\n",
    "              'Sp:', round(self.md['sp'], 3),\n",
    "              'Avg:', round(self.md['snspavg'], 3),\n",
    "              'Acc:', round(self.md['acc'], 3),\n",
    "              'AUC:', round(self.md['auc'], 3), \n",
    "              '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmetrics = MedMetricsCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDR callback by epoch\n",
    "\n",
    "\n",
    "Here we define a set of functions and Keras callback to implement a stochastic gradient descent with restarts (SGDR) using a simple cosine-based learning rate decay with a reset schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a simple, single cosine-based learning rate annealing schedule falling to nearly zero over the \n",
    "# specified number of steps\n",
    "def cos_sgdr(steps_per_cycle):\n",
    "  sched = np.arange(0, 1, 1 / steps_per_cycle)\n",
    "  sched = np.pi * sched\n",
    "  sched = np.cos(sched)\n",
    "  sched = (sched + 1) / 2\n",
    "  return list(sched)\n",
    "\n",
    "# Returns a list of learning rates for multiple cycles; exp=1 makes all cycles same length, exp>1 causes \n",
    "# them to lengthen\n",
    "def cos_sgdr_sched(steps_per_cycle, cycles, exp):\n",
    "  sched = []\n",
    "\n",
    "for cycle in range(cycles):\n",
    "    subsched = []\n",
    "    subsched = cos_sgdr(steps_per_cycle * (exp**cycle))\n",
    "    for value in subsched: sched.append(value)\n",
    "        \n",
    "  return sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schedule for use with learning rate adjustment after each epoch, rather than by batch\n",
    "sgdr_sched_epoch = cos_sgdr_sched(10, 1, 2) \n",
    "plt.plot(range(len(sgdr_sched_epoch)), sgdr_sched_epoch)\n",
    "print('Schedule length =', len(sgdr_sched_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr = 10**-4 # rescale learning rate schedule to max learning rate from finder (below)\n",
    "sgdr_sched_epoch = np.asarray(sgdr_sched_epoch)\n",
    "sgdr_sched_epoch = sgdr_sched_epoch * max_lr\n",
    "print('Starting learning rate =', sgdr_sched_epoch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgdr_sched_cb_fxn(epoch):\n",
    "  return sgdr_sched_epoch[epoch]\n",
    "\n",
    "sgdr_sched_cb = callbacks.LearningRateScheduler(sgdr_sched_cb_fxn, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "Here we train the network using combinations of class weighting / no class weighting, and SGDR / no SGDR. When working with the notebook you should choose one model to train and then move on to the visualization of the training measures in the succeeding sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No class weights, no SGDR\n",
    "history = model.fit_generator(train_generator,\n",
    "                              train_generator.n // batch_size,\n",
    "                              workers=4,\n",
    "                              epochs=20,\n",
    "                              callbacks=[mmetrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No class weights, SGDR\n",
    "history = model.fit_generator(train_generator,\n",
    "                              train_generator.n // batch_size,\n",
    "                              workers=4,\n",
    "                              epochs=len(sgdr_sched_epoch),\n",
    "                              callbacks=[mmetrics, sgdr_sched_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights, no SGDR\n",
    "history = model.fit_generator(train_generator,\n",
    "                              train_generator.n // batch_size,\n",
    "                              workers=4,\n",
    "                              epochs=len(sgdr_sched_epoch),\n",
    "                              class_weight=class_weight,\n",
    "                              callbacks=[mmetrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights, SGDR\n",
    "history = model.fit_generator(train_generator,\n",
    "                              train_generator.n // batch_size,\n",
    "                              workers=4,\n",
    "                              epochs=len(sgdr_sched_epoch),\n",
    "                              class_weight=class_weight,\n",
    "                              callbacks=[mmetrics, sgdr_sched_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard\n",
    "history = model.fit_generator(train_generator,\n",
    "                              train_generator.n // batch_size,\n",
    "                              validation_data=validation_generator,\n",
    "                              validation_steps=validation_generator.n // batch_size,\n",
    "                              workers=4,\n",
    "                              epochs=20,\n",
    "                              class_weight=class_weight,\n",
    "                              callbacks=[mmetrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('incresnetv2_600_sgdr_1_10_epochs_all_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of training statistics\n",
    "\n",
    "Visualizations of training and model performance statistics. Execute cells as appropriate for the trainined model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity / specificity plot w/AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sens = mmetrics.hist['sn']\n",
    "spec = mmetrics.hist['sp']\n",
    "avg = mmetrics.hist['snspavg']\n",
    "roc_auc = mmetrics.hist['auc']\n",
    "    \n",
    "plt.plot(range(len(sens)), sens, 'b', label = 'Sn')\n",
    "plt.plot(range(len(spec)), spec, 'r', label = 'Sp')\n",
    "plt.plot(range(len(avg)), avg, 'bo', label = 'SnSp Avg')\n",
    "plt.plot(range(len(roc_auc)), roc_auc, 'ro', label = 'AUC')\n",
    "plt.xlabel('Epochs')\n",
    "plt.title('Validation Results')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results\n",
    "\n",
    "Run this cell before running any of the below visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(labels, predictions, thresh = 0.5):\n",
    "\n",
    "    md = med_metrics(validation_generator.classes, predictions, thresh=thresh)\n",
    "    sn = md['sn']\n",
    "    sp = md['sp']\n",
    "    snspavg = md['snspavg']\n",
    "    roc_auc = md['auc']\n",
    "    tpr = md['tpr']\n",
    "    fpr = md['fpr']\n",
    "    cm = md['cm']\n",
    "    acc = md['acc']\n",
    "\n",
    "    print('Results at threshold', str(thresh) + ':')\n",
    "    print('Sn =', round(sn * 100, 1), '%')\n",
    "    print('Sp =', round(sp * 100, 1), '%')\n",
    "    print('Avg =', round(snspavg * 100, 1), '%')\n",
    "    print('Acc =', round(acc * 100, 1), '%')\n",
    "    print(cm)\n",
    "\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation w/test time image augmentation; sensitivity, specificity, accuracy\n",
    "\n",
    "Test the model on the validation data set with test time image augmentation (TTA). This should provide a rough lower bound on the model's general purpose performance where the positioning and quality of the input images may not be as precise as those we used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20, \n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.1,\n",
    "    fill_mode='constant',\n",
    "    cval=0)\n",
    "\n",
    "tta_generator = tta_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(height, width), \n",
    "    shuffle=False, \n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    classes = ['norm', 'frac'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 5 # number of times to augment\n",
    "preds = np.zeros((tta_generator.classes.shape[0], repeats))\n",
    "\n",
    "for i in range(repeats):\n",
    "    print('Predicting augmentation #', i + 1, '\\r', end='')\n",
    "    val_preds = model.predict_generator(tta_generator)\n",
    "    preds[:, i] = val_preds[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_preds = preds.max(axis=1)\n",
    "mean_preds = preds.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(validation_generator.classes, max_preds, thresh=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(validation_generator.classes, mean_preds, thresh=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and specificity on validation data w/o TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(validation_generator.classes, val_preds, thresh=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fnames = validation_generator.filenames\n",
    "test_fnames = np.asarray(test_fnames)\n",
    "\n",
    "np.save('test_fnames', test_fnames)\n",
    "np.save('test_classes', validation_generator.classes)\n",
    "np.save('test_preds', val_preds)\n",
    "np.save('test_max_preds', max_preds)\n",
    "np.save('test_mean_preds', mean_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export results to Microsoft Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fnames = np.load('test_fnames.npy')\n",
    "\n",
    "for i in range(len(test_fnames)):\n",
    "    test_fnames[i] = test_fnames[i][16:35]\n",
    "    \n",
    "test_classes = np.load('test_classes.npy')\n",
    "test_preds = np.load('test_preds.npy')\n",
    "test_max_preds = np.load('test_max_preds.npy')\n",
    "test_mean_preds = np.load('test_mean_preds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import numpy as np\n",
    " \n",
    "df = pd.DataFrame({'filenames': test_fnames,\n",
    "                   'labels': test_classes,\n",
    "                   'raw predictions': test_preds[:, 0],\n",
    "                   'max augmented predictions': test_max_preds,\n",
    "                   'mean augmented predictions': test_mean_preds\n",
    "                  })\n",
    " \n",
    "writer = ExcelWriter('final_results.xlsx')\n",
    "df.to_excel(writer, 'Sheet1', index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and loss charts when not using medical metrics callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "val_acc = history.history['val_acc']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(range(len(acc)), val_acc, 'b')\n",
    "plt.plot(range(len(acc)), acc, 'bo')\n",
    "plt.title('Training and validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(acc)), val_loss, 'b')\n",
    "plt.plot(range(len(acc)), loss, 'bo')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Finder\n",
    "\n",
    "Code to find the optimal learning rate using a small sample of images. The optimal learning rate can then be used in prior cells for training on the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('temp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_lr = os.path.join(base, 'lr_finder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen_lr = ImageDataGenerator(rescale = 1. / 255)\n",
    "train_generator_lr = train_datagen_lr.flow_from_directory(train_dir_lr,\n",
    "    class_mode='binary',\n",
    "    target_size=(height,width),\n",
    "    batch_size=batch_size)\n",
    "\n",
    "finder_steps = train_generator_lr.n // batch_size\n",
    "epochs = 50 # number of learning rates to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a schedule of increasing learning rates\n",
    "def lr_finder_sched(steps, base_lr = 10**-10):\n",
    "  finder_sched = np.ones((steps))\n",
    "  finder_sched = finder_sched * base_lr\n",
    "  exponent = np.arange(0, 10, 10/steps)\n",
    "  finder_sched = finder_sched * 10 ** exponent\n",
    "  return finder_sched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_sched = lr_finder_sched(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(lr_sched)), lr_sched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the learning rate for a given epoch\n",
    "def lr_returner(epoch):\n",
    "    return lr_sched[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = callbacks.LearningRateScheduler(lr_returner, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_counts = np.unique(train_generator.classes, return_counts=True)\n",
    "class_weight = {0: train_counts[1] / train_counts[0], 1: 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizers.Adam(lr=0), loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit_generator(train_generator_lr,\n",
    "                              steps_per_epoch=finder_steps,\n",
    "                              epochs=epochs,\n",
    "                              class_weight=class_weight,\n",
    "                              callbacks=[lrate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(lr_sched, history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('temp.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolational neural network class activation heatmap\n",
    "\n",
    "Generates heatmaps that show the areas of an image that maximize a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.3 # for transparency\n",
    "image_source = os.path.join(valid_dir, 'norm')\n",
    "heatmap_path = os.path.join(base, 'heatmaps')\n",
    "fnames = os.listdir(image_source)\n",
    "starting_index = 1980\n",
    "\n",
    "for ind in range(starting_index, len(fnames)):\n",
    "    \n",
    "    print(\"Processing heatmap\", ind, 'of', len(fnames), '\\r', end='')\n",
    "\n",
    "    img = image.load_img(os.path.join(image_source,fnames[ind]), target_size=(height, width))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.\n",
    "\n",
    "    pred = model.predict(x)\n",
    "    frac_output = model.output[:, 0]\n",
    "    last_conv_layer = model.layers[0]\n",
    "\n",
    "    grads = K.gradients(frac_output, last_conv_layer.get_output_at(1))[0]\n",
    "\n",
    "    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "    iterate = K.function([model.input], [pooled_grads, last_conv_layer.get_output_at(1)[0]])\n",
    "    pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "\n",
    "    for i in range(1536):\n",
    "        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "\n",
    "    heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    img = cv2.imread(os.path.join(image_source,fnames[ind]))\n",
    "    \n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    heatmap_fname = 'heatmap_' + '%03d' % int(pred * 100) + '_' + fnames[ind]\n",
    "    heatmap = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "\n",
    "    cv2.imwrite(os.path.join(heatmap_path, heatmap_fname), heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sgdr_sched_epoch = cos_sgdr_sched(10,1,2) at 0.0001 base lr\n",
    "\n",
    "600 resolution, TTA * 4\n",
    "\n",
    "Raw results:\n",
    "Sn = 82.0 %\n",
    "Sp = 94.5 %\n",
    "Avg = 88.25 %\n",
    "\n",
    "Mean augmented results:\n",
    "Sn = 81.3 %\n",
    "Sp = 96.1 %\n",
    "Avg = 88.7 %\n",
    "\n",
    "Minpooled augmented results:\n",
    "Sn = 88.7 %\n",
    "Sp = 89.8 %\n",
    "Avg = 89.25 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*with BN\n",
    "\n",
    "Raw results:\n",
    "Sn = 84.0 %\n",
    "Sp = 93.8 %\n",
    "Avg = 88.9 %\n",
    "\n",
    "Minpooled augmented results:\n",
    "Sn = 88.7 %\n",
    "Sp = 86.2 %\n",
    "Avg = 87.45 %\n",
    "\n",
    "Mean augmented results:\n",
    "Sn = 84.0 %\n",
    "Sp = 94.0 %\n",
    "Avg = 89.0 %\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
